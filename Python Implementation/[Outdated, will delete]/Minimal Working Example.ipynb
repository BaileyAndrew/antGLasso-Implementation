{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83e2aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import dependencies\n",
    "import line_profiler\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import invwishart, wishart\n",
    "from Scripts.utilities import LASSO, generate_confusion_matrices\n",
    "from Scripts.utilities import precision, recall, accuracy\n",
    "from Scripts.utilities import scale_diagonals_to_1, tr_p\n",
    "from Scripts.utilities import kron_sum, kron_sum_diag, kron_prod\n",
    "from Scripts.generate_data import generate_Ys\n",
    "from Scripts.niBiGLasso import niBiGLasso\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f014573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 51, 49)\n",
      "(51, 51)\n",
      "(49, 49)\n"
     ]
    }
   ],
   "source": [
    "Psi_gen, Theta_gen, Ys = generate_Ys(\n",
    "    m=100, # Note that we only need m=3 for good vindication, whereas\n",
    "    n=(n:=51), # m needs to be much larger to get good estimates in general\n",
    "    p=(p:=49), # anyways (in this alg and scalable version).\n",
    "    expected_nonzero_psi=n**2 / 5, # (divide by 5 for sparsity)\n",
    "    expected_nonzero_theta=p**2 / 5,\n",
    "    structure=\"Kronecker Sum\",\n",
    "    posdef_distr=invwishart#invwishart\n",
    ")\n",
    "print(Ys.shape)\n",
    "print(Psi_gen.shape)\n",
    "print(Theta_gen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3be6aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyticBiGLasso(\n",
    "    Ys: \"(m, n, p) batch of m samples of (n, p) matrices\",\n",
    "    beta_1: \"L1 penalty for within-row precision matrix\",\n",
    "    beta_2: \"L2 penalty for within-column precision matrix\",\n",
    "    true_U = None,\n",
    "    true_V = None\n",
    ") -> (\n",
    "    \"(n, n) within-row precision matrix Psi\",\n",
    "    \"(p, p) within-row precision matrix Theta\"\n",
    "):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n, p) = Ys.shape\n",
    "    \n",
    "    # Step one:\n",
    "    # Calculate the empirical covariance matrices\n",
    "    # Uncomment to get unbiased versions\n",
    "    correction = m #m - 1 if m > 0 else m\n",
    "    T_psi: \"(n, n)\" = np.einsum(\"mnp, mlp -> nl\", Ys, Ys) / (correction * n)\n",
    "    T_theta: \"(p, p)\" = np.einsum(\"mnp, mnl -> pl\", Ys, Ys) / (correction * p)\n",
    "        \n",
    "    #return np.linalg.inv(T_psi), np.linalg.inv(T_theta), (T_psi, T_theta)\n",
    "        \n",
    "    assert T_psi.shape == (n, n)\n",
    "    assert T_theta.shape == (p, p)\n",
    "    \n",
    "    # Step two:\n",
    "    # Eigendecompose the scaled precision matrices\n",
    "    I_psi = np.eye(T_psi.shape[0])\n",
    "    J_psi = np.ones(T_psi.shape)\n",
    "    K_psi = (2*p-1)*I_psi + p*(J_psi-I_psi)\n",
    "    T_psi_scaled = T_psi * K_psi\n",
    "    \n",
    "    # theory\n",
    "    T_psi_scaled = scale_diagonals_to_1(T_psi_scaled)\n",
    "    \n",
    "    I_theta = np.eye(T_theta.shape[0])\n",
    "    J_theta = np.ones(T_theta.shape)\n",
    "    K_theta = (2*n-1)*I_theta + n*(J_theta-I_theta)\n",
    "    T_theta_scaled = T_theta * K_theta\n",
    "    \n",
    "     # theory\n",
    "    T_theta_scaled = scale_diagonals_to_1(T_theta_scaled)\n",
    "    \n",
    "    U: \"Eigenvectors of Psi\"\n",
    "    V: \"Eigenvectors of Theta\"\n",
    "    _u, U = np.linalg.eig(T_psi_scaled)\n",
    "    _v, V = np.linalg.eig(T_theta_scaled)\n",
    "    \n",
    "    if true_U is not None:\n",
    "        #U = true_U\n",
    "        # scales to true_U scale\n",
    "        E = np.diag(np.sign(true_U)*np.sqrt(np.diag(np.abs(true_U))))\n",
    "        U = E @ U @ E\n",
    "    if true_V is not None:\n",
    "        #V = true_V\n",
    "        E = np.diag(np.sign(true_V)*np.sqrt(np.diag(np.abs(true_V))))\n",
    "        V = E @ V @ E\n",
    "    \n",
    "    assert U.shape == (n, n)\n",
    "    assert V.shape == (p, p)\n",
    "    \n",
    "    # We can check empirically that our calculation was correct\n",
    "    # This compares lines [1] and [4] of the proof\n",
    "    assert np.isclose(\n",
    "        T_psi - 1 / (2*p) * T_psi * I_psi,\n",
    "        1/p * (T_psi * K_psi) - 1/(2*p) * (T_psi * K_psi) * I_psi\n",
    "    ).all()\n",
    "    assert np.isclose(\n",
    "        T_theta - 1 / (2*n) * T_theta * I_theta,\n",
    "        1/n * (T_theta * K_theta) - 1/(2*n) * (T_theta * K_theta) * I_theta\n",
    "    ).all()\n",
    "    \n",
    "    # Step three:\n",
    "    # Calculate the (m, n, p) Xs tensor\n",
    "    # This matrix is the 'standardized' form of the Ys input,\n",
    "    # and by that we mean that it can be thought of as having\n",
    "    # been drawn from a Kronecker Sum Distribution whose\n",
    "    # precision matrix is the kronecker sum of the eigenvalues\n",
    "    Xs: \"(m, n, p)\" = np.einsum(\"an, mnp, pb -> mab\", U.T, Ys, V)\n",
    "    assert Xs.shape == (m, n, p)\n",
    "    \n",
    "    # To check this did what we expect, let's undo the operation\n",
    "    # on one of the Xs\n",
    "    assert np.isclose(\n",
    "        U @ Xs[0] @ V.T,\n",
    "        Ys[0]\n",
    "    ).all()\n",
    "    \n",
    "    # We can also derive that X@X.T == U.T@Y@Y.T@U\n",
    "    assert np.isclose(\n",
    "        np.einsum(\"mnp, mlp -> nl\", Xs, Xs) / (correction*n),\n",
    "        U.T @ T_psi @ U\n",
    "    ).all()\n",
    "    \n",
    "    # Step four:\n",
    "    # Calculate the reciprocals of the diagonals elements\n",
    "    # of the Sigma matrices, which are the covariance\n",
    "    # matrices for the distribution of the rows of X\n",
    "    \"\"\"\n",
    "    a: \"(n*p,)\" = np.empty((n*p,))\n",
    "    for idx in range(n*p):\n",
    "        # for loops are inefficient, this can be optimized\n",
    "        i = idx % n\n",
    "        j = idx // n\n",
    "        xs_to_sum = Xs[:, i, j]\n",
    "        a[i + j*n] = m / (xs_to_sum * xs_to_sum).sum()\n",
    "    assert a.shape == (n*p,)\n",
    "    \"\"\"\n",
    "    # Sigma^i_j in proof is Sigma[i, j] here\n",
    "    Sigma: \"(n, p)\" = np.empty((n, p))\n",
    "    for i in range(n):\n",
    "        for j in range(p):\n",
    "            # For loops are bad, can be made more efficient\n",
    "            xs_to_sum = Xs[:, i, j]\n",
    "            Sigma[i, j] = m / (xs_to_sum * xs_to_sum).sum()\n",
    "    assert Sigma.shape == (n, p)\n",
    "    \n",
    "    a = Sigma.T.reshape((n*p,))\n",
    "    assert a.shape == (n*p, )\n",
    "    if n > 1 and p > 2:\n",
    "        # Check that `a[i + j*n] == Sigma[i, j]`\n",
    "        assert a[1 + 2*n] == m / (Xs[:, 1, 2] * Xs[:, 1, 2]).sum()\n",
    "        \n",
    "    # Step five:\n",
    "    # Construct the B matrix that relates `a` to the eigenvalues\n",
    "    # Note that this matrix is overdetermined. Its size np by n+p\n",
    "    # and hence has n^2p+np^2 total elements - however with a\n",
    "    # smart argument we should be able to reduce this b/c its\n",
    "    # overdetermined.  Should be reducible to (n+p)^2 elements,\n",
    "    # which does add a O(np) term to the space complexity, but\n",
    "    # this is subsumed by X's O(mnp) and doesn't really matter\n",
    "    # because of the input's O(mnp) anyways.\n",
    "    B: \"(n*p, n+p)\" = np.empty((n*p, n+p))\n",
    "    for row in range(n*p):\n",
    "        i = row % n\n",
    "        j = row // n\n",
    "        B[row, :] = 0\n",
    "        B[row, i] = 1\n",
    "        B[row, n+j] = 1\n",
    "    assert B.shape == (n*p, n+p)\n",
    "    \n",
    "    # Step six:\n",
    "    # Solve the system of linear equations to get the eigenvalues\n",
    "    Ls: \"Stacked eigenvalues\" = np.linalg.lstsq(B, a, rcond=None)[0]\n",
    "    assert Ls.shape == (n+p,)\n",
    "        \n",
    "    # We can use these eigenvalues to double-check the maximum\n",
    "    # likelihood criteria\n",
    "    D = np.linalg.inv(np.diag(kron_sum_diag(Ls[:n], Ls[n:])))\n",
    "    trpD = tr_p(D, p=p)\n",
    "    \"\"\"\n",
    "    print(U @ np.diag(trpD) @ U.T)\n",
    "    print(T_psi_scaled)\n",
    "    # It's not an MLE estimate unless this is true\n",
    "    assert np.isclose(\n",
    "        U @ np.diag(trpD) @ U.T,\n",
    "        T_psi_scaled\n",
    "    ).all()\n",
    "    \"\"\"\n",
    "        \n",
    "    # It seems if you're given random eigenvalues:\n",
    "    Ls[:n] = np.random.random((n,))\n",
    "    Ls[n:] = np.random.random((p,))\n",
    "    # it still does 'well' if you have true U, V\n",
    "        \n",
    "    # Step seven:\n",
    "    # Calculate Psi, Theta, finally!\n",
    "    # (Scaling to 1 is important for LASSO as then all columns\n",
    "    # will be on the same scale)\n",
    "    Psi = scale_diagonals_to_1(U @ np.diag(Ls[:n]) @ U.T)\n",
    "    Theta = scale_diagonals_to_1(V @ np.diag(Ls[n:]) @ V.T)\n",
    "    assert Psi.shape == (n, n)\n",
    "    assert Theta.shape == (p, p)\n",
    "    \n",
    "    # Step eight:\n",
    "    # Perform Lasso regression\n",
    "    # (Could replace by row-wise Lasso regression\n",
    "    # as in the original algorithm)\n",
    "    if beta_1 > 0:\n",
    "        Psi = LASSO(np.eye(Psi.shape[0]), Psi-I_psi, beta_1 / n) + I_psi\n",
    "    if beta_2 > 0:\n",
    "        Theta = LASSO(np.eye(Theta.shape[0]), Theta-I_theta, beta_2 / p) + I_theta\n",
    "    \n",
    "    return Psi, Theta, (U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5deeb9cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m u, U \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meig(Psi_gen)\n\u001b[1;32m      2\u001b[0m v, V \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meig(Theta_gen)\n\u001b[0;32m----> 3\u001b[0m Psi, Theta, extras \u001b[38;5;241m=\u001b[39m \u001b[43manalyticBiGLasso\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mYs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#0.02,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#0.02,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrue_U\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrue_V\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Let's make some assertions to check the proof\u001b[39;00m\n\u001b[1;32m     12\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(kron_sum(Psi_gen, Theta_gen))\n",
      "Input \u001b[0;32mIn [103]\u001b[0m, in \u001b[0;36manalyticBiGLasso\u001b[0;34m(Ys, beta_1, beta_2, true_U, true_V)\u001b[0m\n\u001b[1;32m     59\u001b[0m     E \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39msign(true_V)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39mabs(true_V))))\n\u001b[1;32m     60\u001b[0m     V \u001b[38;5;241m=\u001b[39m E \u001b[38;5;241m@\u001b[39m V \u001b[38;5;241m@\u001b[39m E\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m U\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (n, n)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m V\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (p, p)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# We can check empirically that our calculation was correct\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# This compares lines [1] and [4] of the proof\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "u, U = np.linalg.eig(Psi_gen)\n",
    "v, V = np.linalg.eig(Theta_gen)\n",
    "Psi, Theta, extras = analyticBiGLasso(\n",
    "    Ys,\n",
    "    0,#0.02,\n",
    "    0,#0.02,\n",
    "    true_U=U,\n",
    "    true_V=V\n",
    ")\n",
    "\n",
    "# Let's make some assertions to check the proof\n",
    "W = np.linalg.inv(kron_sum(Psi_gen, Theta_gen))\n",
    "trpW = tr_p(W, p=p)\n",
    "print(trpW)\n",
    "D = np.linalg.inv(np.diag(kron_sum_diag(u, v)))\n",
    "I = np.eye(p)\n",
    "trpD = tr_p(D, p=p)\n",
    "What = (kron_prod(U, I) @ D @ kron_prod(U.T, I))\n",
    "trpWhat = tr_p(What, p=p)\n",
    "\n",
    "# These assertions verify that the trp[W] reduction works\n",
    "# i.e. we can get the eigenvectors!\n",
    "assert np.isclose(\n",
    "    trpW,\n",
    "    trpWhat\n",
    ").all()\n",
    "assert np.isclose(\n",
    "    trpW,\n",
    "    U @ trpD @ U.T\n",
    ").all()\n",
    "\n",
    "# Efficiency\n",
    "print(\"===Psi===\")\n",
    "print(Psi_cm := generate_confusion_matrices(Psi, Psi_gen, mode='Negative'))\n",
    "print(\n",
    "    f\"{precision(Psi_cm)=:.3f}\",\n",
    "    f\"\\n{recall(Psi_cm)=:.3f}\",\n",
    "    f\"\\n{accuracy(Psi_cm)=:.3f}\"\n",
    ")\n",
    "print(\"\\n==Theta==\")\n",
    "print(Theta_cm := generate_confusion_matrices(Theta, Theta_gen, mode='Negative'))\n",
    "print(\n",
    "    f\"{precision(Theta_cm)=:.3f}\",\n",
    "    f\"\\n{recall(Theta_cm)=:.3f}\",\n",
    "    f\"\\n{accuracy(Theta_cm)=:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e4479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Psi===\n",
      "[[  72.  810.]\n",
      " [ 124. 1544.]]\n",
      "precision(Psi_cm)=0.082 \n",
      "recall(Psi_cm)=0.367 \n",
      "accuracy(Psi_cm)=0.634\n",
      "\n",
      "==Theta==\n",
      "[[  72.  792.]\n",
      " [ 146. 1342.]]\n",
      "precision(Theta_cm)=0.083 \n",
      "recall(Theta_cm)=0.330 \n",
      "accuracy(Theta_cm)=0.601\n"
     ]
    }
   ],
   "source": [
    "from Scripts.scBiGLasso import scBiGLasso\n",
    "Psi, Theta = scBiGLasso(\n",
    "    100,\n",
    "    1e-3,\n",
    "    Ys,\n",
    "    0.01,\n",
    "    0.01,\n",
    ")\n",
    "print(\"===Psi===\")\n",
    "print(Psi_cm := generate_confusion_matrices(Psi, Psi_gen, mode='Negative'))\n",
    "print(\n",
    "    f\"{precision(Psi_cm)=:.3f}\",\n",
    "    f\"\\n{recall(Psi_cm)=:.3f}\",\n",
    "    f\"\\n{accuracy(Psi_cm)=:.3f}\"\n",
    ")\n",
    "print(\"\\n==Theta==\")\n",
    "print(Theta_cm := generate_confusion_matrices(Theta, Theta_gen, mode='Negative'))\n",
    "print(\n",
    "    f\"{precision(Theta_cm)=:.3f}\",\n",
    "    f\"\\n{recall(Theta_cm)=:.3f}\",\n",
    "    f\"\\n{accuracy(Theta_cm)=:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "0312c134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psi vindication: 0.9997221875594935\n",
      "Theta vindication: 0.9999964625115909\n",
      "===Psi===\n",
      "[[   4.  184.]\n",
      " [ 206. 2156.]]\n",
      "precision(Psi_cm)=0.021 \n",
      "recall(Psi_cm)=0.019 \n",
      "accuracy(Psi_cm)=0.847\n",
      "\n",
      "==Theta==\n",
      "[[   0.    6.]\n",
      " [ 280. 2066.]]\n",
      "precision(Theta_cm)=0.000 \n",
      "recall(Theta_cm)=0.000 \n",
      "accuracy(Theta_cm)=0.878\n"
     ]
    }
   ],
   "source": [
    "nibig = niBiGLasso()\n",
    "T_psi, T_theta = nibig.get_empiricals(Ys)\n",
    "nibig.fit(T_psi, T_theta)\n",
    "Psi, Theta = nibig.shrink(0.02, 0.02)#0.02, 0.02\n",
    "nibig.print_vindication()\n",
    "print(\"===Psi===\")\n",
    "print(Psi_cm := generate_confusion_matrices(Psi, Psi_gen, mode='Negative'))\n",
    "print(\n",
    "    f\"{precision(Psi_cm)=:.3f}\",\n",
    "    f\"\\n{recall(Psi_cm)=:.3f}\",\n",
    "    f\"\\n{accuracy(Psi_cm)=:.3f}\"\n",
    ")\n",
    "print(\"\\n==Theta==\")\n",
    "print(Theta_cm := generate_confusion_matrices(Theta, Theta_gen, mode='Negative'))\n",
    "print(\n",
    "    f\"{precision(Theta_cm)=:.3f}\",\n",
    "    f\"\\n{recall(Theta_cm)=:.3f}\",\n",
    "    f\"\\n{accuracy(Theta_cm)=:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41bfebe",
   "metadata": {},
   "source": [
    "# Empirical Verification of Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2915b6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43  0.132]\n",
      " [0.014 0.028]]\n",
      "[[0.43  0.132]\n",
      " [0.014 0.028]]\n",
      "[[ True  True]\n",
      " [ True  True]]\n",
      "[[0.819 1.977 0.627]\n",
      " [2.15  4.971 1.566]\n",
      " [0.729 1.693 0.531]]\n",
      "[[0.819 1.977 0.627]\n",
      " [2.15  4.971 1.566]\n",
      " [0.729 1.693 0.531]]\n",
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# Empirically verify Theorem 1\n",
    "# (what are the chances the equations would work out\n",
    "# for randomly selected matrices if it were false?)\n",
    "T = np.random.random((2, 2))\n",
    "I = np.eye(2)\n",
    "J = np.ones((2, 2))\n",
    "K = (2*p-1)*I + p*(J-I)\n",
    "p = 2\n",
    "trpW = T * K\n",
    "\n",
    "# Verify lines 1-4, these should be equal\n",
    "print(T - 1/(2*p) * T * I)\n",
    "print(1/p * trpW - 1/(2*p)*trpW * I)\n",
    "print(np.isclose(T - 1/(2*p) * T * I, 1/p * trpW - 1/(2*p)*trpW * I))\n",
    "\n",
    "from Scripts.utilities import tr_p, kron_prod\n",
    "U = np.random.random((3, 3))\n",
    "D = np.random.random((6, 6))\n",
    "\n",
    "# Verify lines 6-7, these should be equal\n",
    "print(tr_p(kron_prod(U, I) @ D @ kron_prod(U.T, I), p=2))\n",
    "print(U @ tr_p(D, p=2) @ U.T)\n",
    "print(\n",
    "    np.isclose(\n",
    "        tr_p(kron_prod(U, I) @ D @ kron_prod(U.T, I), p=2),\n",
    "        U @ tr_p(D, p=2) @ U.T\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b957f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999.753915090151\n",
      "999.7539150901546\n",
      "True\n",
      "7.299938075612053\n",
      "7.29993807561205\n",
      "True\n",
      "1.5968853534425984\n",
      "1.5968853534425982\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from Scripts.generate_data import generate_sparse_posdef_matrix\n",
    "from Scripts.utilities import kron_sum, kron_sum_diag\n",
    "# Verify lemma 1\n",
    "\n",
    "# First the |Psi kronsum Theta| equality\n",
    "Psi = generate_sparse_posdef_matrix(4, 16)[0]\n",
    "Theta = generate_sparse_posdef_matrix(5, 25)[0]\n",
    "u, U = np.linalg.eigh(Psi)\n",
    "v, V = np.linalg.eigh(Theta)\n",
    "kr_diag = np.diag(kron_sum_diag(u, v))\n",
    "print(np.linalg.det(kron_sum(Psi, Theta)))\n",
    "print(np.linalg.det(kr_diag))\n",
    "print(np.isclose(np.linalg.det(kr_diag), np.linalg.det(kron_sum(Psi, Theta))))\n",
    "\n",
    "# Verify the first trace equality\n",
    "Y = np.random.random((4, 5))\n",
    "X = U.T @ Y @ V\n",
    "print(np.trace(Psi @ Y @ Y.T))\n",
    "print(np.trace(np.diag(u) @ X @ X.T))\n",
    "print(np.isclose(np.trace(Psi @ Y @ Y.T), np.trace(np.diag(u) @ X @ X.T)))\n",
    "\n",
    "# Verify the second trace equality\n",
    "print(np.trace(Theta @ Y.T @ Y))\n",
    "print(np.trace(np.diag(v) @ X.T @ X))\n",
    "print(np.isclose(np.trace(Theta @ Y.T @ Y), np.trace(np.diag(v) @ X.T @ X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f0dbeee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189.74988738109164"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How good is the MLE of U, really?\n",
    "errs = []\n",
    "tries = 1000\n",
    "for i in range(tries):\n",
    "    Psi_gen, Theta_gen, Ys = generate_Ys(\n",
    "        m=(m:=10),\n",
    "        n=(n:=21),\n",
    "        p=(p:=19),\n",
    "        expected_nonzero_psi=n**2 / 5, # (divide by 5 for sparsity)\n",
    "        expected_nonzero_theta=p**2 / 5,\n",
    "        structure=\"Kronecker Sum\",\n",
    "        posdef_distr=invwishart\n",
    "    )\n",
    "    correction = m#-1\n",
    "    T: \"(n, n)\" = np.einsum(\"mnp, mlp -> nl\", Ys, Ys) / (correction * n)\n",
    "    u, U = np.linalg.eig(Psi_gen)\n",
    "    v, V = np.linalg.eig(Theta_gen)\n",
    "    D = np.linalg.inv(np.diag(kron_sum_diag(u, v)))\n",
    "    trpD = tr_p(D, p=p)\n",
    "    #W = np.linalg.inv(kron_sum(Psi_gen, Theta_gen))\n",
    "    #trpW = tr_p(W, p=p)\n",
    "    I = np.eye(U.shape[0])\n",
    "    J = np.ones(U.shape)\n",
    "    K = (2*p-1)*I + p*(J-I)\n",
    "    #print(np.linalg.norm(trpW - T * K, ord='fro'))\n",
    "    diff = U @ trpD @ U.T - T * K\n",
    "    #diff = scale_diagonals_to_1(U @ trpD @ U.T) - scale_diagonals_to_1(T * K)\n",
    "    errs.append(np.linalg.norm(diff, ord='fro'))\n",
    "sum(errs) / tries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
